"""Tests for reduction operations with gradient checking.

This module tests all reduction backward implementations using numerical gradient checking
with finite differences. This is the gold standard for validating gradient computation.

Tests cover:
- Sum reduction along axes
- Mean reduction along axes
- Max reduction (returns indices)
- Min reduction (returns indices)
- Numerical gradient checking using finite differences

Gradient checking formula:
    numerical_grad ≈ (f(x + ε) - f(x - ε)) / (2ε)

All tests validate backward passes produce correct gradient values.
"""

from tests.shared.conftest import (
    assert_close_float,
    assert_equal,
    assert_equal_int,
    assert_shape,
    assert_true,
)
from tests.shared.conftest import TestFixtures
from shared.core.extensor import ExTensor, zeros, ones, zeros_like, ones_like
from shared.core.reduction import (
    sum,
    mean,
    max_reduce,
    min_reduce,
    variance,
    std,
    median,
    percentile,
    sum_backward,
    mean_backward,
    max_reduce_backward,
    min_reduce_backward,
    variance_backward,
    std_backward,
    median_backward,
    percentile_backward,
)
from shared.testing import check_gradient


# ============================================================================
# Sum Reduction Tests
# ============================================================================


fn test_sum_backward_shapes() raises:
    """Test that sum_backward returns correct gradient shape."""
    var shape = List[Int]()
    shape.append(2)
    shape.append(3)
    shape.append(4)

    var x = ones(shape, DType.float32)

    # Reduce along axis 1
    var result = sum(x, axis=1)

    # Gradient matching output shape
    var grad_output = ones_like(result)

    # Backward pass
    var grad_input = sum_backward(grad_output, x, axis=1)

    # Check shape matches input
    var gi_shape = grad_input.shape()
    assert_equal(gi_shape[0], 2)
    assert_equal(gi_shape[1], 3)
    assert_equal(gi_shape[2], 4)


fn test_sum_backward_gradient() raises:
    """Test sum_backward with numerical gradient checking."""
    var shape = List[Int]()
    shape.append(2)
    shape.append(3)

    var x = zeros(shape, DType.float32)

    # Set non-uniform values
    x._data.bitcast[Float32]()[0] = 0.5
    x._data.bitcast[Float32]()[1] = -0.3
    x._data.bitcast[Float32]()[2] = 1.2
    x._data.bitcast[Float32]()[3] = -0.8
    x._data.bitcast[Float32]()[4] = 0.1
    x._data.bitcast[Float32]()[5] = 0.7

    # Forward function wrapper (sum along axis 1)
    fn forward(inp: ExTensor) raises escaping -> ExTensor:
        return sum(inp, axis=1)

    var y = forward(x)
    var grad_out = ones_like(y)

    # Backward function wrapper
    fn backward(grad: ExTensor, inp: ExTensor) raises escaping -> ExTensor:
        return sum_backward(grad, inp, axis=1)

    # Use numerical gradient checking (gold standard)
    # Note: rtol=2e-3 accounts for Float32 precision in sum accumulation
    check_gradient(forward, backward, x, grad_out, rtol=2e-3, atol=1e-6)


# ============================================================================
# Mean Reduction Tests
# ============================================================================


fn test_mean_backward_shapes() raises:
    """Test that mean_backward returns correct gradient shape."""
    var shape = List[Int]()
    shape.append(2)
    shape.append(3)
    shape.append(4)

    var x = ones(shape, DType.float32)

    # Reduce along axis 1
    var result = mean(x, axis=1)

    # Gradient matching output shape
    var grad_output = ones_like(result)

    # Backward pass
    var grad_input = mean_backward(grad_output, x, axis=1)

    # Check shape matches input
    var gi_shape = grad_input.shape()
    assert_equal(gi_shape[0], 2)
    assert_equal(gi_shape[1], 3)
    assert_equal(gi_shape[2], 4)


fn test_mean_backward_gradient() raises:
    """Test mean_backward with numerical gradient checking."""
    var shape = List[Int]()
    shape.append(2)
    shape.append(3)

    var x = zeros(shape, DType.float32)

    # Set non-uniform values
    x._data.bitcast[Float32]()[0] = 0.5
    x._data.bitcast[Float32]()[1] = -0.3
    x._data.bitcast[Float32]()[2] = 1.2
    x._data.bitcast[Float32]()[3] = -0.8
    x._data.bitcast[Float32]()[4] = 0.1
    x._data.bitcast[Float32]()[5] = 0.7

    # Forward function wrapper (mean along axis 1)
    fn forward(inp: ExTensor) raises escaping -> ExTensor:
        return mean(inp, axis=1)

    var y = forward(x)
    var grad_out = ones_like(y)

    # Backward function wrapper
    fn backward(grad: ExTensor, inp: ExTensor) raises escaping -> ExTensor:
        return mean_backward(grad, inp, axis=1)

    # Use numerical gradient checking (gold standard)
    # Note: rtol=2e-3 accounts for Float32 precision in division
    check_gradient(forward, backward, x, grad_out, rtol=2e-3, atol=1e-6)


# ============================================================================
# Max Reduction Tests
# ============================================================================


fn test_max_reduce_backward_shapes() raises:
    """Test that max_reduce_backward returns correct gradient shape."""
    var shape = List[Int]()
    shape.append(3)
    shape.append(4)

    var x = ones(shape, DType.float32)

    # Reduce along axis 1
    var result = max_reduce(x, axis=1)

    # Gradient matching output shape
    var grad_output = ones_like(result)

    # Backward pass
    var grad_input = max_reduce_backward(grad_output, x, axis=1)

    # Check shape matches input
    var gi_shape = grad_input.shape()
    assert_equal(gi_shape[0], 3)
    assert_equal(gi_shape[1], 4)


fn test_max_reduce_backward_gradient() raises:
    """Test max_reduce_backward with numerical gradient checking."""
    var shape = List[Int]()
    shape.append(2)
    shape.append(3)

    var x = zeros(shape, DType.float32)

    # Set non-uniform values
    x._data.bitcast[Float32]()[0] = 0.5
    x._data.bitcast[Float32]()[1] = -0.3
    x._data.bitcast[Float32]()[2] = 1.2
    x._data.bitcast[Float32]()[3] = -0.8
    x._data.bitcast[Float32]()[4] = 0.1
    x._data.bitcast[Float32]()[5] = 0.7

    # Forward function wrapper (max along axis 1)
    fn forward(inp: ExTensor) raises escaping -> ExTensor:
        return max_reduce(inp, axis=1)

    var y = forward(x)
    var grad_out = ones_like(y)

    # Backward function wrapper
    fn backward(grad: ExTensor, inp: ExTensor) raises escaping -> ExTensor:
        return max_reduce_backward(grad, inp, axis=1)

    # Use numerical gradient checking (gold standard)
    # Note: rtol=2e-3 accounts for Float32 precision
    check_gradient(forward, backward, x, grad_out, rtol=2e-3, atol=1e-6)


# ============================================================================
# Min Reduction Tests
# ============================================================================


fn test_min_reduce_backward_shapes() raises:
    """Test that min_reduce_backward returns correct gradient shape."""
    var shape = List[Int]()
    shape.append(3)
    shape.append(4)

    var x = ones(shape, DType.float32)

    # Reduce along axis 1
    var result = min_reduce(x, axis=1)

    # Gradient matching output shape
    var grad_output = ones_like(result)

    # Backward pass
    var grad_input = min_reduce_backward(grad_output, x, axis=1)

    # Check shape matches input
    var gi_shape = grad_input.shape()
    assert_equal(gi_shape[0], 3)
    assert_equal(gi_shape[1], 4)


fn test_min_reduce_backward_gradient() raises:
    """Test min_reduce_backward with numerical gradient checking."""
    var shape = List[Int]()
    shape.append(2)
    shape.append(3)

    var x = zeros(shape, DType.float32)

    # Set non-uniform values
    x._data.bitcast[Float32]()[0] = 0.5
    x._data.bitcast[Float32]()[1] = -0.3
    x._data.bitcast[Float32]()[2] = 1.2
    x._data.bitcast[Float32]()[3] = -0.8
    x._data.bitcast[Float32]()[4] = 0.1
    x._data.bitcast[Float32]()[5] = 0.7

    # Forward function wrapper (min along axis 1)
    fn forward(inp: ExTensor) raises escaping -> ExTensor:
        return min_reduce(inp, axis=1)

    var y = forward(x)
    var grad_out = ones_like(y)

    # Backward function wrapper
    fn backward(grad: ExTensor, inp: ExTensor) raises escaping -> ExTensor:
        return min_reduce_backward(grad, inp, axis=1)

    # Use numerical gradient checking (gold standard)
    # Note: rtol=2e-3 accounts for Float32 precision
    check_gradient(forward, backward, x, grad_out, rtol=2e-3, atol=1e-6)


# ============================================================================
# Variance Reduction Tests
# ============================================================================


fn test_var_forward_uniform() raises:
    """Test variance of uniform values (should be 0)."""
    var shape = List[Int]()
    shape.append(5)
    var x = ones(shape, DType.float32)

    var result = variance(x, axis=-1)
    assert_close_float(result._get_float64(0), 0.0, rtol=1e-5, atol=1e-7)


fn test_var_forward_simple() raises:
    """Test variance with known result."""
    var shape = List[Int]()
    shape.append(3)
    var x = zeros(shape, DType.float32)
    x._data.bitcast[Float32]()[0] = 1.0
    x._data.bitcast[Float32]()[1] = 2.0
    x._data.bitcast[Float32]()[2] = 3.0

    # Mean = 2.0, var = ((1-2)^2 + (2-2)^2 + (3-2)^2) / 3 = 2/3
    var result = variance(x, axis=-1, ddof=0)
    assert_close_float(result._get_float64(0), 2.0 / 3.0, rtol=1e-5, atol=1e-7)


fn test_var_forward_with_ddof() raises:
    """Test sample variance with ddof=1."""
    var shape = List[Int]()
    shape.append(3)
    var x = zeros(shape, DType.float32)
    x._data.bitcast[Float32]()[0] = 1.0
    x._data.bitcast[Float32]()[1] = 2.0
    x._data.bitcast[Float32]()[2] = 3.0

    # Sample variance with ddof=1: var = 2 / 2 = 1.0
    var result = variance(x, axis=-1, ddof=1)
    assert_close_float(result._get_float64(0), 1.0, rtol=1e-5, atol=1e-7)


fn test_var_forward_axis() raises:
    """Test variance along specific axis."""
    var shape = List[Int]()
    shape.append(2)
    shape.append(3)
    var x = zeros(shape, DType.float32)
    x._data.bitcast[Float32]()[0] = 1.0
    x._data.bitcast[Float32]()[1] = 2.0
    x._data.bitcast[Float32]()[2] = 3.0
    x._data.bitcast[Float32]()[3] = 4.0
    x._data.bitcast[Float32]()[4] = 5.0
    x._data.bitcast[Float32]()[5] = 6.0

    var result = variance(x, axis=1, ddof=0)
    var result_shape = result.shape()
    assert_equal(result_shape[0], 2)


fn test_var_backward_shapes() raises:
    """Test that var_backward returns correct gradient shape."""
    var shape = List[Int]()
    shape.append(2)
    shape.append(3)
    var x = zeros(shape, DType.float32)
    for i in range(6):
        x._data.bitcast[Float32]()[i] = Float32(i) + 1.0

    var result = variance(x, axis=1, ddof=0)
    var grad_output = ones_like(result)
    var grad_input = variance_backward(grad_output, x, axis=1, ddof=0)

    var gi_shape = grad_input.shape()
    assert_equal(gi_shape[0], 2)
    assert_equal(gi_shape[1], 3)


fn test_var_backward_gradient() raises:
    """Test var_backward with numerical gradient checking."""
    var shape = List[Int]()
    shape.append(2)
    shape.append(3)
    var x = zeros(shape, DType.float32)
    x._data.bitcast[Float32]()[0] = 0.5
    x._data.bitcast[Float32]()[1] = -0.3
    x._data.bitcast[Float32]()[2] = 1.2
    x._data.bitcast[Float32]()[3] = -0.8
    x._data.bitcast[Float32]()[4] = 0.1
    x._data.bitcast[Float32]()[5] = 0.7

    fn forward(inp: ExTensor) raises escaping -> ExTensor:
        return variance(inp, axis=1, ddof=0)

    var y = forward(x)
    var grad_out = ones_like(y)

    fn backward(grad: ExTensor, inp: ExTensor) raises escaping -> ExTensor:
        return variance_backward(grad, inp, axis=1, ddof=0)

    check_gradient(forward, backward, x, grad_out, rtol=2e-3, atol=1e-6)


# ============================================================================
# Standard Deviation Tests
# ============================================================================


fn test_std_forward_simple() raises:
    """Test standard deviation with known result."""
    var shape = List[Int]()
    shape.append(3)
    var x = zeros(shape, DType.float32)
    x._data.bitcast[Float32]()[0] = 1.0
    x._data.bitcast[Float32]()[1] = 2.0
    x._data.bitcast[Float32]()[2] = 3.0

    # std = sqrt(var) = sqrt(2/3)
    var result = std(x, axis=-1, ddof=0)
    var expected = (2.0 / 3.0) ** 0.5
    assert_close_float(result._get_float64(0), expected, rtol=1e-5, atol=1e-7)


fn test_std_backward_gradient() raises:
    """Test std_backward with numerical gradient checking."""
    var shape = List[Int]()
    shape.append(2)
    shape.append(3)
    var x = zeros(shape, DType.float32)
    x._data.bitcast[Float32]()[0] = 0.5
    x._data.bitcast[Float32]()[1] = 0.3
    x._data.bitcast[Float32]()[2] = 1.2
    x._data.bitcast[Float32]()[3] = 0.8
    x._data.bitcast[Float32]()[4] = 0.1
    x._data.bitcast[Float32]()[5] = 0.7

    fn forward(inp: ExTensor) raises escaping -> ExTensor:
        return std(inp, axis=1, ddof=0)

    var y = forward(x)
    var grad_out = ones_like(y)

    fn backward(grad: ExTensor, inp: ExTensor) raises escaping -> ExTensor:
        return std_backward(grad, inp, axis=1, ddof=0)

    check_gradient(forward, backward, x, grad_out, rtol=2e-3, atol=1e-6)


# ============================================================================
# Median Tests
# ============================================================================


fn test_median_forward_odd() raises:
    """Test median with odd count."""
    var shape = List[Int]()
    shape.append(5)
    var x = zeros(shape, DType.float32)
    x._data.bitcast[Float32]()[0] = 3.0
    x._data.bitcast[Float32]()[1] = 1.0
    x._data.bitcast[Float32]()[2] = 4.0
    x._data.bitcast[Float32]()[3] = 2.0
    x._data.bitcast[Float32]()[4] = 5.0

    var result = median(x, axis=-1)
    assert_close_float(result._get_float64(0), 3.0, rtol=1e-5, atol=1e-7)


fn test_median_forward_even() raises:
    """Test median with even count (average of two middle values)."""
    var shape = List[Int]()
    shape.append(4)
    var x = zeros(shape, DType.float32)
    x._data.bitcast[Float32]()[0] = 1.0
    x._data.bitcast[Float32]()[1] = 2.0
    x._data.bitcast[Float32]()[2] = 3.0
    x._data.bitcast[Float32]()[3] = 4.0

    var result = median(x, axis=-1)
    assert_close_float(result._get_float64(0), 2.5, rtol=1e-5, atol=1e-7)


fn test_median_backward_shapes() raises:
    """Test that median_backward returns correct gradient shape."""
    var shape = List[Int]()
    shape.append(2)
    shape.append(3)
    var x = zeros(shape, DType.float32)
    for i in range(6):
        x._data.bitcast[Float32]()[i] = Float32(i) + 1.0

    var result = median(x, axis=1)
    var grad_output = ones_like(result)
    var grad_input = median_backward(grad_output, x, axis=1)

    var gi_shape = grad_input.shape()
    assert_equal(gi_shape[0], 2)
    assert_equal(gi_shape[1], 3)


# ============================================================================
# Percentile Tests
# ============================================================================


fn test_percentile_forward_p50() raises:
    """Test that 50th percentile equals median."""
    var shape = List[Int]()
    shape.append(5)
    var x = zeros(shape, DType.float32)
    x._data.bitcast[Float32]()[0] = 1.0
    x._data.bitcast[Float32]()[1] = 2.0
    x._data.bitcast[Float32]()[2] = 3.0
    x._data.bitcast[Float32]()[3] = 4.0
    x._data.bitcast[Float32]()[4] = 5.0

    var result = percentile(x, 50.0, axis=-1)
    assert_close_float(result._get_float64(0), 3.0, rtol=1e-5, atol=1e-7)


fn test_percentile_forward_p0_p100() raises:
    """Test that 0th and 100th percentiles equal min and max."""
    var shape = List[Int]()
    shape.append(5)
    var x = zeros(shape, DType.float32)
    x._data.bitcast[Float32]()[0] = 1.0
    x._data.bitcast[Float32]()[1] = 2.0
    x._data.bitcast[Float32]()[2] = 3.0
    x._data.bitcast[Float32]()[3] = 4.0
    x._data.bitcast[Float32]()[4] = 5.0

    var p0 = percentile(x, 0.0, axis=-1)
    assert_close_float(p0._get_float64(0), 1.0, rtol=1e-5, atol=1e-7)

    var p100 = percentile(x, 100.0, axis=-1)
    assert_close_float(p100._get_float64(0), 5.0, rtol=1e-5, atol=1e-7)


fn test_percentile_backward_shapes() raises:
    """Test that percentile_backward returns correct gradient shape."""
    var shape = List[Int]()
    shape.append(2)
    shape.append(3)
    var x = zeros(shape, DType.float32)
    for i in range(6):
        x._data.bitcast[Float32]()[i] = Float32(i) + 1.0

    var result = percentile(x, 50.0, axis=1)
    var grad_output = ones_like(result)
    var grad_input = percentile_backward(grad_output, x, 50.0, axis=1)

    var gi_shape = grad_input.shape()
    assert_equal(gi_shape[0], 2)
    assert_equal(gi_shape[1], 3)


# ============================================================================
# Main Test Runner
# ============================================================================


fn main() raises:
    """Run all reduction tests."""
    print("Running reduction tests...")

    # Sum reduction tests
    test_sum_backward_shapes()
    print("✓ test_sum_backward_shapes")

    test_sum_backward_gradient()
    print("✓ test_sum_backward_gradient")

    # Mean reduction tests
    test_mean_backward_shapes()
    print("✓ test_mean_backward_shapes")

    test_mean_backward_gradient()
    print("✓ test_mean_backward_gradient")

    # Max reduction tests
    test_max_reduce_backward_shapes()
    print("✓ test_max_reduce_backward_shapes")

    test_max_reduce_backward_gradient()
    print("✓ test_max_reduce_backward_gradient")

    # Min reduction tests
    test_min_reduce_backward_shapes()
    print("✓ test_min_reduce_backward_shapes")

    test_min_reduce_backward_gradient()
    print("✓ test_min_reduce_backward_gradient")

    # Variance tests
    test_var_forward_uniform()
    print("✓ test_var_forward_uniform")

    test_var_forward_simple()
    print("✓ test_var_forward_simple")

    test_var_forward_with_ddof()
    print("✓ test_var_forward_with_ddof")

    test_var_forward_axis()
    print("✓ test_var_forward_axis")

    test_var_backward_shapes()
    print("✓ test_var_backward_shapes")

    test_var_backward_gradient()
    print("✓ test_var_backward_gradient")

    # Standard deviation tests
    test_std_forward_simple()
    print("✓ test_std_forward_simple")

    test_std_backward_gradient()
    print("✓ test_std_backward_gradient")

    # Median tests
    test_median_forward_odd()
    print("✓ test_median_forward_odd")

    test_median_forward_even()
    print("✓ test_median_forward_even")

    test_median_backward_shapes()
    print("✓ test_median_backward_shapes")

    # Percentile tests
    test_percentile_forward_p50()
    print("✓ test_percentile_forward_p50")

    test_percentile_forward_p0_p100()
    print("✓ test_percentile_forward_p0_p100")

    test_percentile_backward_shapes()
    print("✓ test_percentile_backward_shapes")

    print("\nAll reduction tests passed!")
